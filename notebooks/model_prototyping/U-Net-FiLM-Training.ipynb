{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bd2856",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nahubn1/Hybrid-Robot-Navigation-System/blob/main/notebooks/model_prototyping/U-Net-FiLM-Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "WPy4XKZqRhXI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WPy4XKZqRhXI",
    "outputId": "d51ddc1b-eaf9-4dad-bf37-c2608f0e75b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gwVlGFyGSAVS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwVlGFyGSAVS",
    "outputId": "5aa1ba28-ac14-4bc8-c59e-15fabc4d65fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects:  20% (1/5)\u001b[K\r",
      "remote: Counting objects:  40% (2/5)\u001b[K\r",
      "remote: Counting objects:  60% (3/5)\u001b[K\r",
      "remote: Counting objects:  80% (4/5)\u001b[K\r",
      "remote: Counting objects: 100% (5/5)\u001b[K\r",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects:  20% (1/5)\u001b[K\r",
      "remote: Compressing objects:  40% (2/5)\u001b[K\r",
      "remote: Compressing objects:  60% (3/5)\u001b[K\r",
      "remote: Compressing objects:  80% (4/5)\u001b[K\r",
      "remote: Compressing objects: 100% (5/5)\u001b[K\r",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects:  20% (1/5)\r",
      "Unpacking objects:  40% (2/5)\r",
      "Unpacking objects:  60% (3/5)\r",
      "Unpacking objects:  80% (4/5)\r",
      "Unpacking objects: 100% (5/5)\r",
      "Unpacking objects: 100% (5/5), 1.34 KiB | 688.00 KiB/s, done.\n",
      "From https://github.com/nahubn1/Hybrid-Robot-Navigation-System\n",
      "   2423c22..a0758bf  main       -> origin/main\n",
      "Updating 2423c22..a0758bf\n",
      "Fast-forward\n",
      " .../model_prototyping/U-Net-FiLM-Training.ipynb    | 33 \u001b[32m++++++++++++++++++\u001b[m\u001b[31m----\u001b[m\n",
      " 1 file changed, 28 insertions(+), 5 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "COLAB_PROJECT_ROOT = '/content/Thesis_Project'\n",
    "if not os.path.exists(COLAB_PROJECT_ROOT):\n",
    "  !git clone https://github.com/nahubn1/Hybrid-Robot-Navigation-System {COLAB_PROJECT_ROOT}\n",
    "os.chdir(COLAB_PROJECT_ROOT) # Change directory into the project\n",
    "!git pull # Ensure it's the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "QMwq7BT7SIqp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMwq7BT7SIqp",
    "outputId": "b4f34740-c758-42c5-ca22-d0a42e97dd6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pybullet==3.2.7 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 1)) (3.2.7)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: networkx>=3.1 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 4)) (3.5)\n",
      "Requirement already satisfied: pytest>=8.0 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 5)) (8.3.5)\n",
      "Requirement already satisfied: matplotlib>=3.8 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 6)) (3.10.0)\n",
      "Requirement already satisfied: pillow>=10.0 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 7)) (11.2.1)\n",
      "Requirement already satisfied: tqdm>=4.66 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 9)) (1.15.3)\n",
      "Requirement already satisfied: filelock>=3.13 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 10)) (3.18.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=8.0->-r environment/requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest>=8.0->-r environment/requirements.txt (line 5)) (24.2)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=8.0->-r environment/requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8->-r environment/requirements.txt (line 6)) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r environment/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "uwyQkuZ5SNig",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwyQkuZ5SNig",
    "outputId": "8846bad2-e95c-4342-cfec-b4fd89fec1bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link 'data/data': File exists\n",
      "ln: failed to create symbolic link 'models/models': File exists\n",
      "ln: failed to create symbolic link 'results/results': File exists\n",
      "✅ Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "\n",
    "# Define Drive paths\n",
    "DRIVE_DATA_PATH = '/content/drive/MyDrive/Thesis_DNN_Planner/data'\n",
    "DRIVE_MODELS_PATH = '/content/drive/MyDrive/Thesis_DNN_Planner/models'\n",
    "DRIVE_RESULTS_PATH = '/content/drive/MyDrive/Thesis_DNN_Planner/results'\n",
    "\n",
    "# Link Drive storage to local cloned directories\n",
    "if not os.path.islink('data'):\n",
    "    !ln -s {DRIVE_DATA_PATH} data\n",
    "if not os.path.islink('models'):\n",
    "    !ln -s {DRIVE_MODELS_PATH} models\n",
    "if not os.path.islink('results'):\n",
    "    !ln -s {DRIVE_RESULTS_PATH} results\n",
    "\n",
    "print(\"✅ Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd1d68",
   "metadata": {},
   "source": [
    "### Phase 2, Task 2.5: Full Pipeline Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289ecf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from dnn_guidance.data_loader import PathfindingDataset\n",
    "from dnn_guidance.model import UNetFiLM\n",
    "from dnn_guidance.loss import DiceFocalLoss\n",
    "\n",
    "# Create a temporary minimal dataset\n",
    "_temp_root = Path('tmp_sanity_data')\n",
    "_samples = _temp_root/'samples'\n",
    "_gt = _temp_root/'gt'\n",
    "_samples.mkdir(parents=True, exist_ok=True)\n",
    "_gt.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "grid = np.zeros((200, 200), dtype=np.uint8)\n",
    "grid[0, 0] = 8  # start\n",
    "grid[-1, -1] = 9  # goal\n",
    "np.savez(_samples/'sample0.npz', map=grid, clearance=2.0, step_size=8.0, config=np.array([]))\n",
    "heatmap = np.zeros((200, 200), dtype=np.float32)\n",
    "np.savez(_gt/'sample0.npz', heatmap=heatmap)\n",
    "\n",
    "# Build dataset and dataloader\n",
    " dataset = PathfindingDataset(_samples, _gt)\n",
    "loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNetFiLM().to(device)\n",
    "criterion = DiceFocalLoss()\n",
    "\n",
    "(grid_batch, robot_batch), target_batch = next(iter(loader))\n",
    "grid_batch = grid_batch.to(device)\n",
    "robot_batch = robot_batch.to(device)\n",
    "target_batch = target_batch.to(device)\n",
    "\n",
    "logits = model(grid_batch, robot_batch)\n",
    "print('Logits shape:', logits.shape)\n",
    "loss = criterion(logits, target_batch)\n",
    "print('Loss:', loss.item())\n",
    "\n",
    "assert logits.shape == (1, 1, 200, 200)\n",
    "assert loss.dim() == 0\n",
    "\n",
    "loss.backward()\n",
    "print('✅ Phase 2 Sanity Check Passed: Model, data pipeline, and loss function are fully integrated and functional.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc521c",
   "metadata": {},
   "source": [
    "### Phase 3: Experiment Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41c564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "CONFIG_PATH = 'configs/dnn/unet_film_v1_baseline.yaml'\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "print(f'Configuration loaded from {CONFIG_PATH}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ecb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = cfg.get('seed', 0)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "print(f'Random seed set to {seed}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1588cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from dnn_guidance.data_loader import PathfindingDataset, _pair_files\n",
    "\n",
    "samples_dir = Path(cfg['samples_dir'])\n",
    "gt_dir = Path(cfg['ground_truth_dir'])\n",
    "all_pairs = _pair_files(samples_dir, gt_dir)\n",
    "train_pairs, val_pairs = train_test_split(all_pairs, test_size=cfg['val_split'], random_state=seed)\n",
    "train_dataset = PathfindingDataset(samples_dir, gt_dir, augment=True)\n",
    "val_dataset = PathfindingDataset(samples_dir, gt_dir, augment=False)\n",
    "train_dataset.pairs = train_pairs\n",
    "val_dataset.pairs = val_pairs\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True, num_workers=cfg['num_workers'])\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg['batch_size'], shuffle=False, num_workers=cfg['num_workers'])\n",
    "print('DataLoaders created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnn_guidance.model import UNetFiLM\n",
    "from dnn_guidance.config import UNetConfig\n",
    "\n",
    "model_cfg = UNetConfig.from_yaml('configs/dnn/unet_film.yaml')\n",
    "model = UNetFiLM(model_cfg)\n",
    "device = torch.device(cfg['device'])\n",
    "model = model.to(device)\n",
    "print(f'Model loaded onto {device}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "opt_name = cfg['optimizer']['name']\n",
    "optimizer_cls = getattr(optim, opt_name)\n",
    "optimizer = optimizer_cls(model.parameters(), lr=cfg['optimizer']['lr'], weight_decay=cfg['optimizer']['weight_decay'])\n",
    "scheduler = None\n",
    "if cfg['scheduler']['name'] == 'CosineAnnealing':\n",
    "    t_max = cfg['epochs'] - cfg['scheduler']['warmup_epochs']\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max)\n",
    "print('Optimizer and Scheduler initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnn_guidance.loss import DiceFocalLoss\n",
    "\n",
    "loss_cfg = cfg['loss']\n",
    "loss_fn = DiceFocalLoss(dice_weight=loss_cfg['dice_weight'], focal_weight=loss_cfg['focal_weight'], focal_gamma=loss_cfg['focal_gamma'])\n",
    "print('Loss function initialized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c056a23",
   "metadata": {},
   "source": [
    "### Phase 4: Training Loop with Monitoring and Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275102c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler\n",
    "from dnn_guidance.trainer import train_one_epoch, validate_one_epoch\n",
    "\n",
    "# Directories for logging and checkpoints\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_root = Path(cfg['log_dir'])\n",
    "log_dir = log_root / f\"{cfg['run_name']}_{timestamp}\"\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "ckpt_dir = Path(cfg['checkpoints_dir'])\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_model_path = ckpt_dir / f\"{cfg['run_name']}_best_model.pth\"\n",
    "latest_ckpt = ckpt_dir / f\"{cfg['run_name']}_latest.pth\"\n",
    "\n",
    "scaler = GradScaler(enabled=cfg.get('use_amp', True))\n",
    "best_dice = -1.0\n",
    "epochs_no_improve = 0\n",
    "start_epoch = 0\n",
    "\n",
    "# Resume from checkpoint if available\n",
    "if latest_ckpt.exists():\n",
    "    ckpt = torch.load(latest_ckpt, map_location=device)\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer'])\n",
    "    if scheduler and ckpt.get('scheduler'):\n",
    "        scheduler.load_state_dict(ckpt['scheduler'])\n",
    "    scaler.load_state_dict(ckpt.get('scaler', {}))\n",
    "    start_epoch = ckpt.get('epoch', 0) + 1\n",
    "    best_dice = ckpt.get('best_dice', -1.0)\n",
    "    epochs_no_improve = ckpt.get('epochs_no_improve', 0)\n",
    "    print(f\"Resumed from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, cfg['epochs']):\n",
    "    print(f\"--- Epoch {epoch+1}/{cfg['epochs']} ---\")\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device, scaler)\n",
    "    val_loss, val_dice = validate_one_epoch(model, val_loader, loss_fn, device)\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    writer.add_scalar('loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('dice/val', val_dice, epoch)\n",
    "    writer.add_scalar('lr', lr, epoch)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "    improved = val_dice > best_dice\n",
    "    if improved:\n",
    "        best_dice = val_dice\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\"New best model found, saving checkpoint...\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epochs...\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'scheduler': scheduler.state_dict() if scheduler else None,\n",
    "        'scaler': scaler.state_dict(),\n",
    "        'best_dice': best_dice,\n",
    "        'epochs_no_improve': epochs_no_improve\n",
    "    }, latest_ckpt)\n",
    "\n",
    "    if epochs_no_improve >= cfg['early_stop_patience']:\n",
    "        print('Early stopping triggered.')\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
