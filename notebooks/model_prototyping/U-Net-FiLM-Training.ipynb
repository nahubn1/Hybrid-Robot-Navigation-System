{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nahubn1/Hybrid-Robot-Navigation-System/blob/main/notebooks/model_prototyping/U-Net-FiLM-Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "WPy4XKZqRhXI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPy4XKZqRhXI",
        "outputId": "2801f4cd-cee2-4251-a17f-033a09f466f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "gwVlGFyGSAVS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwVlGFyGSAVS",
        "outputId": "1a5a07fb-7f6c-4b84-b4f7-3a1993c41d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/Thesis_Project'...\n",
            "remote: Enumerating objects: 707, done.\u001b[K\n",
            "remote: Counting objects: 100% (115/115), done.\u001b[K\n",
            "remote: Compressing objects: 100% (98/98), done.\u001b[K\n",
            "remote: Total 707 (delta 46), reused 17 (delta 17), pack-reused 592 (from 2)\u001b[K\n",
            "Receiving objects: 100% (707/707), 420.56 KiB | 1.68 MiB/s, done.\n",
            "Resolving deltas: 100% (385/385), done.\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "COLAB_PROJECT_ROOT = '/content/Thesis_Project'\n",
        "if not os.path.exists(COLAB_PROJECT_ROOT):\n",
        "  !git clone https://github.com/nahubn1/Hybrid-Robot-Navigation-System {COLAB_PROJECT_ROOT}\n",
        "os.chdir(COLAB_PROJECT_ROOT) # Change directory into the project\n",
        "!git pull # Ensure it's the latest version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QMwq7BT7SIqp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMwq7BT7SIqp",
        "outputId": "7d2a987d-3137-4fb5-9b9c-0889d73c03a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet==3.2.7 (from -r environment/requirements.txt (line 1))\n",
            "  Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: networkx>=3.1 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 4)) (3.5)\n",
            "Requirement already satisfied: pytest>=8.0 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 5)) (8.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.8 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: pillow>=10.0 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 7)) (11.2.1)\n",
            "Requirement already satisfied: tqdm>=4.66 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 8)) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.11 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 9)) (1.15.3)\n",
            "Requirement already satisfied: filelock>=3.13 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 10)) (3.18.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from -r environment/requirements.txt (line 11)) (2.6.0+cu124)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=8.0->-r environment/requirements.txt (line 5)) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest>=8.0->-r environment/requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=8.0->-r environment/requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.8->-r environment/requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r environment/requirements.txt (line 11)) (4.14.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r environment/requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r environment/requirements.txt (line 11)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r environment/requirements.txt (line 11)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r environment/requirements.txt (line 11)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r environment/requirements.txt (line 11)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0->-r environment/requirements.txt (line 11))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r environment/requirements.txt (line 11)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->-r environment/requirements.txt (line 11)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->-r environment/requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.8->-r environment/requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->-r environment/requirements.txt (line 11)) (3.0.2)\n",
            "Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m141.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r environment/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "uwyQkuZ5SNig",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwyQkuZ5SNig",
        "outputId": "19e51e4e-775a-4fbf-d55d-448939102ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Environment setup complete.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "# Add src to path\n",
        "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
        "\n",
        "# Define Drive paths\n",
        "DRIVE_DATA_PATH = '/content/drive/MyDrive/Thesis_DNN_Planner/data'\n",
        "DRIVE_MODELS_PATH = '/content/drive/MyDrive/Thesis_DNN_Planner/models'\n",
        "DRIVE_RESULTS_PATH = '/content/drive/MyDrive/Thesis_DNN_Planner/results'\n",
        "\n",
        "# Link Drive storage to local cloned directories\n",
        "if not os.path.islink('data'):\n",
        "    !ln -s {DRIVE_DATA_PATH} data\n",
        "if not os.path.islink('models'):\n",
        "    !ln -s {DRIVE_MODELS_PATH} models\n",
        "if not os.path.islink('results'):\n",
        "    !ln -s {DRIVE_RESULTS_PATH} results\n",
        "\n",
        "print(\"✅ Environment setup complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47bd1d68",
      "metadata": {
        "id": "47bd1d68"
      },
      "source": [
        "### Phase 2, Task 2.5: Full Pipeline Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "289ecf05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "289ecf05",
        "outputId": "cac59443-a44a-450b-adf1-a2ef3689dea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([1, 1, 200, 200])\n",
            "Loss: 0.6198154091835022\n",
            "✅ Phase 2 Sanity Check Passed: Model, data pipeline, and loss function are fully integrated and functional.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from dnn_guidance.data_loader import PathfindingDataset\n",
        "from dnn_guidance.model import UNetFiLM\n",
        "from dnn_guidance.loss import DiceFocalLoss\n",
        "\n",
        "# Create a temporary minimal dataset\n",
        "_temp_root = Path('tmp_sanity_data')\n",
        "_samples = _temp_root/'samples'\n",
        "_gt = _temp_root/'gt'\n",
        "_samples.mkdir(parents=True, exist_ok=True)\n",
        "_gt.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "grid = np.zeros((200, 200), dtype=np.uint8)\n",
        "grid[0, 0] = 8  # start\n",
        "grid[-1, -1] = 9  # goal\n",
        "np.savez(_samples/'sample0.npz', map=grid, clearance=2.0, step_size=8.0, config=np.array([]))\n",
        "heatmap = np.zeros((200, 200), dtype=np.float32)\n",
        "np.savez(_gt/'sample0.npz', heatmap=heatmap)\n",
        "\n",
        "# Build dataset and dataloader\n",
        "dataset = PathfindingDataset(_samples, _gt)\n",
        "loader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNetFiLM().to(device)\n",
        "criterion = DiceFocalLoss()\n",
        "\n",
        "(grid_batch, robot_batch), target_batch = next(iter(loader))\n",
        "grid_batch = grid_batch.to(device)\n",
        "robot_batch = robot_batch.to(device)\n",
        "target_batch = target_batch.to(device)\n",
        "\n",
        "logits = model(grid_batch, robot_batch)\n",
        "print('Logits shape:', logits.shape)\n",
        "loss = criterion(logits, target_batch)\n",
        "print('Loss:', loss.item())\n",
        "\n",
        "assert logits.shape == (1, 1, 200, 200)\n",
        "assert loss.dim() == 0\n",
        "\n",
        "loss.backward()\n",
        "print('✅ Phase 2 Sanity Check Passed: Model, data pipeline, and loss function are fully integrated and functional.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ecc521c",
      "metadata": {
        "id": "9ecc521c"
      },
      "source": [
        "### Phase 3: Experiment Setup and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3d41c564",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d41c564",
        "outputId": "5418f9b9-a115-45a4-e44c-bb092992a93b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded from configs/dnn/unet_film_v1_baseline.yaml.\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "CONFIG_PATH = 'configs/dnn/unet_film_v1_baseline.yaml'\n",
        "with open(CONFIG_PATH, 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "print(f'Configuration loaded from {CONFIG_PATH}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "011ecb42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "011ecb42",
        "outputId": "9bca5592-beb4-41c5-9052-f4ebc1c778f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to 42.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "seed = cfg.get('seed', 0)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "print(f'Random seed set to {seed}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ce1588cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ce1588cf",
        "outputId": "685999e4-7551-44ae-a583-825008eb8204"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dnn_guidance'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3404827644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdnn_guidance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPathfindingDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pair_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dnn_guidance'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from dnn_guidance.data_loader import PathfindingDataset, _pair_files\n",
        "from pathlib import Path\n",
        "\n",
        "samples_dir = Path(DRIVE_DATA_PATH + '/' + cfg['samples_dir'])\n",
        "gt_dir = Path(DRIVE_DATA_PATH + '/' + cfg['ground_truth_dir'])\n",
        "all_pairs = _pair_files(samples_dir, gt_dir)\n",
        "train_pairs, val_pairs = train_test_split(all_pairs, test_size=cfg['val_split'], random_state=seed)\n",
        "train_dataset = PathfindingDataset(samples_dir, gt_dir, augment=True)\n",
        "val_dataset = PathfindingDataset(samples_dir, gt_dir, augment=False)\n",
        "train_dataset.pairs = train_pairs\n",
        "val_dataset.pairs = val_pairs\n",
        "train_loader = DataLoader(train_dataset, batch_size=cfg['batch_size'], shuffle=True, num_workers=cfg['num_workers'])\n",
        "val_loader = DataLoader(val_dataset, batch_size=cfg['batch_size'], shuffle=False, num_workers=cfg['num_workers'])\n",
        "print('DataLoaders created.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9bdc5072",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bdc5072",
        "outputId": "5a2b7143-a9c2-4c8e-9252-b9f970462e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded onto cuda.\n"
          ]
        }
      ],
      "source": [
        "from dnn_guidance.model import UNetFiLM\n",
        "from dnn_guidance.config import UNetConfig\n",
        "\n",
        "model_cfg = UNetConfig.from_yaml('configs/dnn/unet_film.yaml')\n",
        "model = UNetFiLM(model_cfg)\n",
        "device = torch.device(cfg['device'])\n",
        "model = model.to(device)\n",
        "print(f'Model loaded onto {device}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dae9fbbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dae9fbbf",
        "outputId": "b382d654-eadf-47a3-83ca-de57e9903bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer and Scheduler initialized.\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "opt_name = cfg['optimizer']['name']\n",
        "optimizer_cls = getattr(optim, opt_name)\n",
        "optimizer = optimizer_cls(model.parameters(), lr=cfg['optimizer']['lr'], weight_decay=cfg['optimizer']['weight_decay'])\n",
        "scheduler = None\n",
        "if cfg['scheduler']['name'] == 'CosineAnnealing':\n",
        "    t_max = cfg['epochs'] - cfg['scheduler']['warmup_epochs']\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max)\n",
        "print('Optimizer and Scheduler initialized.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5262d268",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5262d268",
        "outputId": "6b149dff-b121-42ac-eaf7-9504ba9dbb9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function initialized.\n"
          ]
        }
      ],
      "source": [
        "from dnn_guidance.loss import DiceFocalLoss\n",
        "\n",
        "loss_cfg = cfg['loss']\n",
        "loss_fn = DiceFocalLoss(dice_weight=loss_cfg['dice_weight'], focal_weight=loss_cfg['focal_weight'], focal_gamma=loss_cfg['focal_gamma'])\n",
        "print('Loss function initialized.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c056a23",
      "metadata": {
        "id": "2c056a23"
      },
      "source": [
        "### Phase 4: Training Loop with Monitoring and Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "275102c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "275102c5",
        "outputId": "e4ae6180-0840-419d-875c-f9570a9209b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-20-1112439998.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=cfg.get('use_amp', True))\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.cuda.amp import GradScaler\n",
        "from dnn_guidance.trainer import train_one_epoch, validate_one_epoch\n",
        "\n",
        "# Directories for logging and checkpoints\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "log_root = Path(cfg['log_dir'])\n",
        "log_dir = log_root / f\"{cfg['run_name']}_{timestamp}\"\n",
        "log_dir.mkdir(parents=True, exist_ok=True)\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "ckpt_dir = Path(cfg['checkpoints_dir'])\n",
        "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "best_model_path = ckpt_dir / f\"{cfg['run_name']}_best_model.pth\"\n",
        "latest_ckpt = ckpt_dir / f\"{cfg['run_name']}_latest.pth\"\n",
        "\n",
        "scaler = GradScaler(enabled=cfg.get('use_amp', True))\n",
        "best_dice = -1.0\n",
        "epochs_no_improve = 0\n",
        "start_epoch = 0\n",
        "\n",
        "# Resume from checkpoint if available\n",
        "if latest_ckpt.exists():\n",
        "    ckpt = torch.load(latest_ckpt, map_location=device)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    if scheduler and ckpt.get('scheduler'):\n",
        "        scheduler.load_state_dict(ckpt['scheduler'])\n",
        "    scaler.load_state_dict(ckpt.get('scaler', {}))\n",
        "    start_epoch = ckpt.get('epoch', 0) + 1\n",
        "    best_dice = ckpt.get('best_dice', -1.0)\n",
        "    epochs_no_improve = ckpt.get('epochs_no_improve', 0)\n",
        "    print(f\"Resumed from epoch {start_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9762b52f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9762b52f",
        "outputId": "fa4ec7a9-50a1-41a5-d655-cf663a420d17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Epoch 1/50 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/content/Thesis_Project/src/dnn_guidance/data_loader.py\", line 83, in __getitem__\n    heatmap_tensor = torch.from_numpy(heatmap[None, ...])\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) \n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-1202397014.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Epoch {epoch+1}/{cfg['epochs']} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Thesis_Project/src/dnn_guidance/trainer.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, loss_fn, device, scaler)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mrobot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/content/Thesis_Project/src/dnn_guidance/data_loader.py\", line 83, in __getitem__\n    heatmap_tensor = torch.from_numpy(heatmap[None, ...])\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) \n"
          ]
        }
      ],
      "source": [
        "for epoch in range(start_epoch, cfg['epochs']):\n",
        "    print(f\"--- Epoch {epoch+1}/{cfg['epochs']} ---\")\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device, scaler)\n",
        "    val_loss, val_dice = validate_one_epoch(model, val_loader, loss_fn, device)\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    writer.add_scalar('loss/train', train_loss, epoch)\n",
        "    writer.add_scalar('loss/val', val_loss, epoch)\n",
        "    writer.add_scalar('dice/val', val_dice, epoch)\n",
        "    writer.add_scalar('lr', lr, epoch)\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f} | LR: {lr:.6f}\")\n",
        "\n",
        "    improved = val_dice > best_dice\n",
        "    if improved:\n",
        "        best_dice = val_dice\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(\"New best model found, saving checkpoint...\")\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"No improvement for {epochs_no_improve} epochs...\")\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'scheduler': scheduler.state_dict() if scheduler else None,\n",
        "        'scaler': scaler.state_dict(),\n",
        "        'best_dice': best_dice,\n",
        "        'epochs_no_improve': epochs_no_improve\n",
        "    }, latest_ckpt)\n",
        "\n",
        "    if epochs_no_improve >= cfg['early_stop_patience']:\n",
        "        print('Early stopping triggered.')\n",
        "        break\n",
        "\n",
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}